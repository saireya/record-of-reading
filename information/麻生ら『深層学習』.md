# 麻生ら[『深層学習』](urn:isbn:476490487X)

# 1章
**深層学習**: *多層ニューラルネット*を用いた機械学習の総称
- 人間の知識には、*明示的に表現することが困難*なものが多い
	- e.g. 文字や顔のパターン認識、運動の学習
- [Hinton 2006](http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf)がきっかけ
- 観測データに内在する本質的構造、特に*階層化された潜在構造*を捉えた**内部表現**をデータから学習
- 少数の特徴が抽出できれば、より簡単に学習可能になる
- 観測データに潜在的な構造が存在するため、**次元削減**が有効
	- e.g. 主成分分析・因子分析・独立成分分析
- **醜いアヒルの子定理**: 対象を表現する特徴を選択せずに、表現の近さに基づいて分類することは不可能
- **no free lunch定理**: すべてのタスクに対し他より優れた性能をもつ万能学習アルゴリズムは存在しない

特徴工学のアプローチ
0. 対象とする情報やタスクに対する人間の理解に基づいて、*人手で内部表現をデザイン*
0. *データに基づいて良い内部表現を学習*させる(**表現学習**)

**ニューラルネットワーク**: *脳神経系*を基にした情報処理の数理モデル
- ニューロンをノードとしてモデル化し、複数のノードを結合
- 入力$$x'$$を重み$$w'$$で処理し、閾値$$\theta$$で出力するニューロンは、$$y = a(x \cdot w - \theta)$$と表せる
- $$x := (1, x'), w := (-\theta, w')$$とおくと、$$y = a(x \cdot w)$$
- $$a$$を**活性化関数(出力関数)**という

**階層型ニューラルネット**: **入力層**・**隠れ(中間)層**・**出力層**からなる
- 入力層は入力信号をそのまま出力($$y^{(1)} = x$$)
- **受容野**: あるノードに結合する(直前とは限らない)前層のノードの集合

# 2章: 深層ボルツマンマシン(RBM)
**生成モデル**$$p_g(X)$$: 観測データ点$$X$$の確率的な生成規則を規定

**Markov確率場(MRF)**: 無向グラフ上の確率モデル $$p(X = x) = e^{-\Phi(x)} / Z$$
- **エネルギー関数** $$\Phi(x) := - \sum \phi_i(x_i) - \sum \psi_{ij}(x_i, x_j)$$
- 第1項: *各ノード*に割り当てられたエネルギー。ノードごとの*変数の値の偏り*を表現
- 第2項: *ノード間結合*に割り当てられたエネルギー。*変数間の関連性*を表現
- エネルギー$$\Phi(x)$$を低くする入力$$x$$を良い入力とする ⇒ $$p(x)$$は大きくなり、出現頻度が高く
- $$Z := \sum e^{-\Phi(x)}$$は**規格化定数**
- **グラフィカルモデル**: グラフ表現と確率モデルを対応させたモデル

**ボルツマンマシン(BM)**: $$\phi_i(x_i) := b_i x_i, \psi_{ij}(x_i, x_j) := w_{ij} x_i x_j$$とする
- エネルギー関数は$$\Phi(x) := - \sum b_i x_i - \sum w_{ij} x_i x_j$$
- $$\theta = \{b, w\}$$がBMのパラメータ
	- **バイアスパラメータ** $$b$$: $$b_i > 0$$ ⇒ $$x_i = 1$$のとき確率がより大きくなる
	- **結合パラメータ** $$w$$: $$w_{ij} > 0$$ ⇒ $$x_i, x_j$$とも1のとき確率がより大きくなる
	- 無向グラフなので$$w_{ij} = w_{ji}$$($$w$$は対称行列)、自己ループはないので$$w_{ii} = 0$$

BMの学習: $$X = (V, H)$$とし、可視変数と隠れ変数を別に扱う
- **可視(観測)変数** $$V_i$$: 観測データに対応する変数
- **隠れ(潜在)変数** $$H_i$$: 観測データに対応しない変数
	- 欠落した観測データの代わりに隠れ変数を利用
	- 学習モデルの表現能力を向上するために隠れ変数を利用
	- 隠れ変数を増やすとモデルは複雑化し、*十分な数の隠れ変数により任意の分布を表現可能*(p.59)
- **モデル誤差**: 学習モデルの表現能力が低いと、実際の生成モデルとの間に誤差を生じる

**制限ボルツマンマシン(RBM)**: 完全2部グラフ上のBM
- 2部グラフなので、**可視層**と**隠れ層**が分離
- エネルギー関数は$$\Phi(x) := - \sum b_i v_i - \sum c_i h_i - \sum w_{ij} v_i h_j$$
	- $$b$$は可視変数の、$$c$$は隠れ変数のバイアスパラメータ
	- $$v_i, v_j$$や$$h_i, h_j$$のときは、その間の結合がないので$$w_{ij} = 0$$
- *完全2部グラフなので、一方の層の確率変数が固定されると、他方の層は互いに独立*(**条件付き独立**)

**深層ボルツマンマシン(DBM)**: 複数の隠れ層と1つの可視層(第0層)をもつRBM
- **事前学習**: DBMをRBMに分けて学習
	- 最初は第0層(可視層)と第1層のRBMで学習
	- 次は、学習で生成したサンプル点(**特徴点**)を用いて第1層を可視層とみなし、第2層とのRBMで学習

# 3章: 事前学習
- **教師あり学習**: 入力$$x$$と正解$$t$$の組の集合$$\{(x_i, t_i)\}$$で学習
	- 入力と出力の間の*関数の学習*
- **教師なし学習**: 入力$$x$$の集合$$\{x_i\}$$のみで学習
	- 入力の*分布の学習*
	- 入力と出力の*同時分布を学習させれば、教師あり学習*も実現可能

**自己符号化器(autoencoder)**: **符号化器**と**復号器**を直接繋いだ*砂時計型*のニューラルネット
- 一旦、入力の次元を減らすため、内部では低次元の特徴で表される
- **符号化器(encoder)** $$h^{(L)} = f(x)$$: 入力層から最も次元の低い中間層まで
- **復号器(decoder)** $$y = g(h^{(L)})$$: そこから出力層まで
- 各層の*パラメータは、符号化器と復号器で対称*とする

# 5章: 画像認識
**畳み込みニューラルネット(CNN)**: **畳み込み層**と**プーリング層**を交互に積み重ねたニューラルネット

# 6章: 音声認識
**回帰結合ニューラルネット(RNN)**: *出力が次の時刻の入力*となるニューラルネット

# 7章: 自然言語処理
**分布仮説**: 単語そのものには潜在した意味がなく、その*単語の使われ方によって意味が生まれる*
- 単語の意味を事前に与える必要がなく、*文脈から学習*させることが可能


